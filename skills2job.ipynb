{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# skill2skill\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from -r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from -r requirements.txt (line 2)) (1.23.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from -r requirements.txt (line 3)) (3.8.1)\n",
      "Requirement already satisfied: requests in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from -r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: gensim in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from -r requirements.txt (line 5)) (4.3.2)\n",
      "Requirement already satisfied: cohere in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from -r requirements.txt (line 6)) (4.27)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 1)) (2.5)\n",
      "Requirement already satisfied: click in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from nltk->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from nltk->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from nltk->-r requirements.txt (line 3)) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\my pc\\appdata\\roaming\\python\\python39\\site-packages (from nltk->-r requirements.txt (line 3)) (4.66.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from requests->-r requirements.txt (line 4)) (2023.7.22)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from gensim->-r requirements.txt (line 5)) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from gensim->-r requirements.txt (line 5)) (6.4.0)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.0 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from cohere->-r requirements.txt (line 6)) (3.8.6)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from cohere->-r requirements.txt (line 6)) (2.2.1)\n",
      "Requirement already satisfied: fastavro==1.8.2 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from cohere->-r requirements.txt (line 6)) (1.8.2)\n",
      "Requirement already satisfied: importlib_metadata<7.0,>=6.0 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from cohere->-r requirements.txt (line 6)) (6.8.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\my pc\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 6)) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 6)) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 6)) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 6)) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from aiohttp<4.0,>=3.0->cohere->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages (from importlib_metadata<7.0,>=6.0->cohere->-r requirements.txt (line 6)) (3.16.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\my pc\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk->-r requirements.txt (line 3)) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\miniconda3\\envs\\tf\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import csv\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# import multiprocessing \n",
    "# import os\n",
    "# import pickle\n",
    "\n",
    "# from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertical_concat = pd.read_csv(\"datasets\\dice_jobs_0.csv\")\n",
    "# for data in os.listdir('datasets')[1:-1]:\n",
    "#     vertical_concat = pd.concat([vertical_concat,pd.read_csv(os.path.join('datasets',data))], axis = 0)\n",
    "# vertical_concat = vertical_concat.dropna().reset_index(drop=True).drop(columns=['job_id'])\n",
    "# vertical_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertical_concat.to_csv('main_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('main_data.csv').drop(columns=['Unnamed: 0'])\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens=[]\n",
    "# for temp in data['job_description'].map(lambda x : x.split(' ')).values:\n",
    "#     tokens += temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab, index = {}, 1  # start indexing from 1\n",
    "# vocab['<pad>'] = 0  # add a padding token\n",
    "# for token in tokens:\n",
    "#   if token not in vocab:\n",
    "#     vocab[token] = index\n",
    "#     index += 1\n",
    "# vocab_size = len(vocab)\n",
    "# inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "# print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_sequence = [vocab[word] for word in tokens]\n",
    "# print(example_sequence[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "#   # Elements of each training example are appended to these lists.\n",
    "#   targets, contexts, labels = [], [], []\n",
    "\n",
    "#   # Build the sampling table for `vocab_size` tokens.\n",
    "#   sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "#   # Iterate over all sequences (sentences) in the dataset.\n",
    "#   for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "#     # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "#     positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "#           sequence,\n",
    "#           vocabulary_size=vocab_size,\n",
    "#           sampling_table=sampling_table,\n",
    "#           window_size=window_size,\n",
    "#           negative_samples=0)\n",
    "\n",
    "#     # Iterate over each positive skip-gram pair to produce training examples\n",
    "#     # with a positive context word and negative samples.\n",
    "#     for target_word, context_word in positive_skip_grams:\n",
    "#       context_class = tf.expand_dims(\n",
    "#           tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "#       negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "#           true_classes=context_class,\n",
    "#           num_true=1,\n",
    "#           num_sampled=num_ns,\n",
    "#           unique=True,\n",
    "#           range_max=vocab_size,\n",
    "#           seed=seed,\n",
    "#           name=\"negative_sampling\")\n",
    "\n",
    "#       # Build context and label vectors (for one target word)\n",
    "#       negative_sampling_candidates = tf.expand_dims(\n",
    "#           negative_sampling_candidates, 1)\n",
    "\n",
    "#       context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "#       label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "#       # Append each element from the training example to global lists.\n",
    "#       targets.append(target_word)\n",
    "#       contexts.append(context)\n",
    "#       labels.append(label)\n",
    "\n",
    "#   return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRIAL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'sample_extract//dice_jobs_0.csv'\n",
    "# data_must_have = pd.read_csv(data_path)\n",
    "# del data_must_have['job_id']\n",
    "\n",
    "# print(len(data_must_have))\n",
    "# data_must_have = data_must_have.drop_duplicates(subset=['job_description', 'job_title'], keep='last')\n",
    "# data_must_have = data_must_have[data_must_have[\"job_title\"] != 0]\n",
    "# print(len(data_must_have))\n",
    "# data_must_have['Count'] = data_must_have.groupby('job_title')['job_description'].transform(pd.Series.value_counts)\n",
    "# data_must_have.drop_duplicates(inplace=True)\n",
    "# data_must_have['job_description'] = data_must_have['job_description'].str.lower()\n",
    "# data_must_have['job_description'] = data_must_have['job_description'].str.replace(' ' ,'_') #map(lambda x: str(x).split(' ') if type(x)==str else x)\n",
    "# data_must_have['job_title'] = data_must_have['job_title'].str.lower()\n",
    "# # gr_df_keywordname = data_must_have.groupby('keyword_name')['job_title'].apply(list)\n",
    "# gr_df_jobtitle = data_must_have.groupby('job_title')['job_description'].apply(list)\n",
    "\n",
    "# gr_df_jobtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gr_df_jobtitle[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must_have_data = []\n",
    "# for vector_list in gr_df_jobtitle:\n",
    "#     xx = list(set(vector_list))\n",
    "#     if xx not in must_have_data:\n",
    "#         must_have_data.append(xx)\n",
    "\n",
    "# len(must_have_data)\n",
    "# must_have_data[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data = pd.read_csv(data_path, index_col='job_id')\n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# musthave_dice_naruki_model_200d = word2vec.Word2Vec(data['job_description'], \n",
    "#           workers=multiprocessing.cpu_count(),  # Number of threads to run in parallel\n",
    "#           size=200, \n",
    "#           min_count=5, \n",
    "#           window=10, \n",
    "#           sample = 1e-3,  # Downsample setting for frequent words\n",
    "#           iter=4,\n",
    "#           sg =1\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = 'main_data.csv'\n",
    "# data_must_have = pd.read_csv(data_path)\n",
    "# # del data_must_have['job_id']\n",
    "\n",
    "# print(len(data_must_have))\n",
    "# data_must_have = data_must_have.drop_duplicates(subset=['job_description', 'job_title'], keep='last')\n",
    "# data_must_have = data_must_have[data_must_have[\"job_title\"] != 0]\n",
    "# print(len(data_must_have))\n",
    "# data_must_have['Count'] = data_must_have.groupby('job_title')['job_description'].transform(pd.Series.value_counts)\n",
    "# data_must_have.drop_duplicates(inplace=True)\n",
    "# data_must_have['job_description'] = data_must_have['job_description'].str.lower()\n",
    "# data_must_have['job_description'] = data_must_have['job_description'].map(lambda x: str(x).split(' ') if type(x)==str else x)\n",
    "# data_must_have['job_title'] = data_must_have['job_title'].str.lower()\n",
    "# # # gr_df_keywordname = data_must_have.groupby('keyword_name')['job_title'].apply(list)\n",
    "# gr_df_jobtitle = data_must_have.groupby('job_title')['job_description'].apply(list)\n",
    "\n",
    "# gr_df_jobtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab, index = {}, 1  # start indexing from 1\n",
    "# vocab['<pad>'] = 0  # add a padding token\n",
    "# for token in gr_df_jobtitle[0]:\n",
    "#   print(token)\n",
    "#   if token not in vocab:\n",
    "#     vocab[token] = index\n",
    "#     index += 1\n",
    "# vocab_size = len(vocab)\n",
    "# print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gr_df_jobtitle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import string\n",
    "# from nltk.corpus import stopwords \n",
    "\n",
    "# def softmax(x):\n",
    "# \t\"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "# \te_x = np.exp(x - np.max(x))\n",
    "# \treturn e_x / e_x.sum()\n",
    "\n",
    "# class word2vec(object):\n",
    "# \tdef __init__(self):\n",
    "# \t\tself.N = 10\n",
    "# \t\tself.X_train = []\n",
    "# \t\tself.y_train = []\n",
    "# \t\tself.window_size = 2\n",
    "# \t\tself.alpha = 0.001\n",
    "# \t\tself.words = []\n",
    "# \t\tself.word_index = {}\n",
    "\n",
    "# \tdef initialize(self,V,data):\n",
    "# \t\tself.V = V\n",
    "# \t\tself.W = np.random.uniform(-0.8, 0.8, (self.V, self.N))\n",
    "# \t\tself.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V))\n",
    "\t\t\n",
    "# \t\tself.words = data\n",
    "# \t\tfor i in range(len(data)):\n",
    "# \t\t\tself.word_index[data[i]] = i\n",
    "\n",
    "\t\n",
    "# \tdef feed_forward(self,X):\n",
    "# \t\tself.h = np.dot(self.W.T,X).reshape(self.N,1)\n",
    "# \t\tself.u = np.dot(self.W1.T,self.h)\n",
    "# \t\t#print(self.u)\n",
    "# \t\tself.y = softmax(self.u) \n",
    "# \t\treturn self.y\n",
    "\t\t\n",
    "# \tdef backpropagate(self,x,t):\n",
    "# \t\te = self.y - np.asarray(t).reshape(self.V,1)\n",
    "# \t\t# e.shape is V x 1\n",
    "# \t\tdLdW1 = np.dot(self.h,e.T)\n",
    "# \t\tX = np.array(x).reshape(self.V,1)\n",
    "# \t\tdLdW = np.dot(X, np.dot(self.W1,e).T)\n",
    "# \t\tself.W1 = self.W1 - self.alpha*dLdW1\n",
    "# \t\tself.W = self.W - self.alpha*dLdW\n",
    "\t\t\n",
    "# \tdef train(self,epochs):\n",
    "# \t\tfor x in range(1,epochs):\t \n",
    "# \t\t\tself.loss = 0\n",
    "# \t\t\tfor j in range(len(self.X_train)):\n",
    "# \t\t\t\tself.feed_forward(self.X_train[j])\n",
    "# \t\t\t\tself.backpropagate(self.X_train[j],self.y_train[j])\n",
    "# \t\t\t\tC = 0\n",
    "# \t\t\t\tfor m in range(self.V):\n",
    "# \t\t\t\t\tif(self.y_train[j][m]):\n",
    "# \t\t\t\t\t\tself.loss += -1*self.u[m][0]\n",
    "# \t\t\t\t\t\tC += 1\n",
    "# \t\t\t\tself.loss += C*np.log(np.sum(np.exp(self.u)))\n",
    "# \t\t\tprint(\"epoch \",x, \" loss = \",self.loss)\n",
    "# \t\t\tself.alpha *= 1/( (1+self.alpha*x) )\n",
    "\t\t\t\n",
    "# \tdef predict(self,word,number_of_predictions):\n",
    "# \t\tif word in self.words:\n",
    "# \t\t\tindex = self.word_index[word]\n",
    "# \t\t\tX = [0 for i in range(self.V)]\n",
    "# \t\t\tX[index] = 1\n",
    "# \t\t\tprediction = self.feed_forward(X)\n",
    "# \t\t\toutput = {}\n",
    "# \t\t\tfor i in range(self.V):\n",
    "# \t\t\t\toutput[prediction[i][0]] = i\n",
    "\t\t\t\n",
    "# \t\t\ttop_context_words = []\n",
    "# \t\t\tfor k in sorted(output,reverse=True):\n",
    "# \t\t\t\ttop_context_words.append(self.words[output[k]])\n",
    "# \t\t\t\tif(len(top_context_words)>=number_of_predictions):\n",
    "# \t\t\t\t\tbreak\n",
    "\t\n",
    "# \t\t\treturn top_context_words\n",
    "# \t\telse:\n",
    "# \t\t\tprint(\"Word not found in dictionary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing(corpus):\n",
    "# \tstop_words = set(stopwords.words('english')) \n",
    "# \ttraining_data = []\n",
    "# \tsentences = corpus#.split(\".\")\n",
    "# \tfor i in range(len(sentences)):\n",
    "# \t\tsentences[i] = sentences[i].strip()\n",
    "# \t\tsentence = sentences[i].split()\n",
    "# \t\tx = [word.strip(string.punctuation) for word in sentence\n",
    "# \t\t\t\t\t\t\t\t\tif word not in stop_words]\n",
    "# \t\tx = [word.lower() for word in x]\n",
    "# \t\ttraining_data.append(x)\n",
    "# \treturn training_data\n",
    "\t\n",
    "\n",
    "# def prepare_data_for_training(sentences,w2v):\n",
    "# \tdata = {}\n",
    "# \tfor sentence in sentences:\n",
    "# \t\tfor word in sentence:\n",
    "# \t\t\tif word not in data:\n",
    "# \t\t\t\tdata[word] = 1\n",
    "# \t\t\telse:\n",
    "# \t\t\t\tdata[word] += 1\n",
    "# \tV = len(data)\n",
    "# \tdata = sorted(list(data.keys()))\n",
    "# \tvocab = {}\n",
    "# \tfor i in range(len(data)):\n",
    "# \t\tvocab[data[i]] = i\n",
    "\t\n",
    "# \t#for i in range(len(words)):\n",
    "# \tfor sentence in sentences:\n",
    "# \t\tfor i in range(len(sentence)):\n",
    "# \t\t\tcenter_word = [0 for x in range(V)]\n",
    "# \t\t\tcenter_word[vocab[sentence[i]]] = 1\n",
    "# \t\t\tcontext = [0 for x in range(V)]\n",
    "\t\t\t\n",
    "# \t\t\tfor j in range(i-w2v.window_size,i+w2v.window_size):\n",
    "# \t\t\t\tif i!=j and j>=0 and j<len(sentence):\n",
    "# \t\t\t\t\tcontext[vocab[sentence[j]]] += 1\n",
    "# \t\t\tw2v.X_train.append(center_word)\n",
    "# \t\t\tw2v.y_train.append(context)\n",
    "# \tw2v.initialize(V,data)\n",
    "\n",
    "# \treturn w2v.X_train,w2v.y_train \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import multiprocessing \n",
    "import csv\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       minimum years experience sdet role mobile test...\n",
       "1       need candidates practical karate api experienc...\n",
       "2       position summary geico seeking experienced sen...\n",
       "3       skills experience plus nice necessarily requir...\n",
       "4       client seeking services experienced applicatio...\n",
       "                              ...                        \n",
       "1362    location hoboken nj salary tbd description cli...\n",
       "1363    job title java full stack developer location m...\n",
       "1364    position net full stack developer location rem...\n",
       "1365    client well known global insurance company see...\n",
       "1366    normal business hours monday friday hours week...\n",
       "Name: job_description, Length: 1367, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Getting job descriptions only\n",
    "data = pd.read_csv(\"main_data.csv\")\n",
    "corpus = data['job_description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating skills vocabulary using data from naukri (as done in skill2vec)\n",
    "skills_tokens = []\n",
    "with open('naukri_skill_full.txt') as naukri:\n",
    "    next(naukri)\n",
    "    past = 0; temp=[]\n",
    "    for n,i in enumerate(naukri):\n",
    "        current = int(i.split(',')[0])\n",
    "        skill = i.lower().split(',')[1].strip()\n",
    "        if past < current:\n",
    "            past = current\n",
    "            skills_tokens.append(temp)\n",
    "            temp=[]\n",
    "        else:\n",
    "            temp.append(skill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ceaser',\n",
       "  'caad',\n",
       "  'design',\n",
       "  'piping',\n",
       "  'thermal power plants',\n",
       "  'boiler',\n",
       "  'turbine'],\n",
       " ['drawing', 'design', 'detail engineering', 'architecture', 'design']]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186720000, 265143200)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training Word2Vec\n",
    "epochs = 32\n",
    "model = Word2Vec(sentences=skills_tokens, vector_size=1000, window=10,\n",
    "                  min_count=5, workers=multiprocessing.cpu_count(), sg=1,\n",
    "                  max_vocab_size = None\n",
    "                  #, sample\n",
    "                  )\n",
    "model.train(corpus, total_examples=len(corpus), epochs=epochs )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to process given array of skills to match those in model vocabulary\n",
    "def process_input_skills(array, unique_words=model.wv.key_to_index):\n",
    "    return [word for word in array if word in unique_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer']\n"
     ]
    }
   ],
   "source": [
    "input_array = ['computer']\n",
    "input_array = process_input_skills(input_array)\n",
    "print(input_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computers', 0.8954794406890869),\n",
       " ('logical', 0.8750045895576477),\n",
       " ('stream', 0.8696491718292236),\n",
       " ('drive', 0.8649522066116333),\n",
       " ('excellent communication skill', 0.8638783693313599),\n",
       " ('phone', 0.8630484938621521),\n",
       " ('talented', 0.852419912815094),\n",
       " ('top', 0.8500362634658813),\n",
       " ('knowledge of computer', 0.8499969244003296),\n",
       " ('verbal', 0.8498958945274353)]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = model.wv.most_similar(input_array, topn=10)  # get other similar words\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big data']\n"
     ]
    }
   ],
   "source": [
    "input_array = ['big data']\n",
    "input_array = process_input_skills(input_array)\n",
    "print(input_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bigdata', 0.9148853421211243),\n",
       " ('spark', 0.9147384166717529),\n",
       " ('big data analytics', 0.9041957259178162),\n",
       " ('hadoop', 0.9018380045890808),\n",
       " ('data science', 0.8834220767021179),\n",
       " ('mahout', 0.8794143795967102),\n",
       " ('hive', 0.8766750693321228),\n",
       " ('data engineering', 0.8750425577163696),\n",
       " ('weka', 0.8731489777565002),\n",
       " ('bi tools', 0.8719069361686707)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = model.wv.most_similar(input_array, topn=10)  # get other similar words\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "input_array = [x.strip() for x in ['leadership ','project management','time management ','communication ','collaboration ',\n",
    "                             'creativity ','innovation ','strategy ','marketing ','networking ','social media ',\n",
    "                             'technology ','computer literacy ','microsoft office ','google workspace ','design ',\n",
    "                             'art ','video editing ','photography ','animation ','app development ','coding ',\n",
    "                             'website development ','seo ','analytics ','research ','copywriting ','content creation ','advertising ',\n",
    "                             'event planning ','sales ','negotiation ','customer service ','motivation ','teamwork ','energy ','ambition ',\n",
    "                             'initiative ','organization ','planning ','attention to detail ','accountability ','flexibility ','determination ',\n",
    "                             'selfmotivation ','research ','analytical skills','problemsolving','decisionmaking','time management']]\n",
    "print(len(input_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "input_array = process_input_skills(input_array)\n",
    "print(len(input_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leadership', 'project management', 'time management', 'communication', 'creativity', 'innovation', 'strategy', 'marketing', 'networking', 'social media', 'technology', 'computer literacy', 'microsoft office', 'design', 'art', 'video editing', 'photography', 'animation', 'app development', 'coding', 'website development', 'seo', 'analytics', 'research', 'copywriting', 'content creation', 'advertising', 'event planning', 'sales', 'negotiation', 'customer service', 'motivation', 'energy', 'initiative', 'organization', 'planning', 'attention to detail', 'flexibility', 'research', 'analytical skills', 'time management']\n"
     ]
    }
   ],
   "source": [
    "print(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marketing services', 0.9491106867790222),\n",
       " ('creative services', 0.942703366279602),\n",
       " ('agencies', 0.9380879402160645),\n",
       " ('community development', 0.9375516176223755),\n",
       " ('management marketing', 0.9354873299598694),\n",
       " ('producer', 0.9324954152107239),\n",
       " ('fans', 0.9288585782051086),\n",
       " ('marketing campaign', 0.9288384318351746),\n",
       " ('sharing', 0.9286950826644897),\n",
       " ('highly motivated', 0.9278717637062073)]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(input_array, topn=10)  # get other similar words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Embedded words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save(\"wordvectors\\word2vec.wordvectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and testing saved embedded words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = KeyedVectors.load(\"wordvectors\\word2vec.wordvectors\", mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marketing services', 0.9491106867790222),\n",
       " ('creative services', 0.942703366279602),\n",
       " ('agencies', 0.9380879402160645),\n",
       " ('community development', 0.9375516176223755),\n",
       " ('management marketing', 0.9354873299598694),\n",
       " ('producer', 0.9324954152107239),\n",
       " ('fans', 0.9288585782051086),\n",
       " ('marketing campaign', 0.9288384318351746),\n",
       " ('sharing', 0.9286950826644897),\n",
       " ('highly motivated', 0.9278717637062073)]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(input_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('elastic', 0.9838289618492126),\n",
       " ('no sql', 0.9778910279273987),\n",
       " ('neo4j', 0.9776978492736816),\n",
       " ('flask', 0.9746628999710083),\n",
       " ('mariadb', 0.9745484590530396),\n",
       " ('phython', 0.9736986756324768),\n",
       " ('backend development', 0.9736780524253845),\n",
       " ('go', 0.9719542860984802),\n",
       " ('rabbit mq', 0.9718587398529053),\n",
       " ('crawling', 0.9711820483207703)]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('backend developer', topn=10)  # get other similar words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
